{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7fec6eaf2f28>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)': /simple/contractions/\u001b[0m\r\n",
      "\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7fec6eaf29b0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)': /simple/contractions/\u001b[0m\r\n",
      "\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7fec6eaf2b00>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)': /simple/contractions/\u001b[0m\r\n",
      "\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7fec6eaf2e48>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)': /simple/contractions/\u001b[0m\r\n",
      "\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7fec6eaf24e0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)': /simple/contractions/\u001b[0m\r\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement Contractions (from versions: none)\u001b[0m\r\n",
      "\u001b[31mERROR: No matching distribution found for Contractions\u001b[0m\r\n",
      "[nltk_data] Error loading all: <urlopen error [Errno -3] Temporary\n",
      "[nltk_data]     failure in name resolution>\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'contractions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8deab73dbcce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municodedata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcontractions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;31m#import inflect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'contractions'"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Jun  4 12:22:27 2019\n",
    "\n",
    "@author: abhalla2\n",
    "\"\"\"\n",
    "\n",
    "!pip install -U Contractions\n",
    "import nltk\n",
    "nltk.download('all')\n",
    "\n",
    "\n",
    "#Importing various libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "import matplotlib.pyplot as plt\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "#import inflect\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "from sklearn.utils import shuffle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Bidirectional\n",
    "import keras \n",
    "from keras.layers import LSTM, CuDNNLSTM\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "\n",
    "#Ignoring the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(action = 'ignore') \n",
    "\n",
    "\n",
    "#Reading the data\n",
    "df2 = pd.read_csv('../input/training/data_combined_train.csv')\n",
    "X = df2.iloc[:, 0:1].values\n",
    "X_train = []\n",
    "for i in X:\n",
    "    for j in i:\n",
    "        X_train.append(str(j))\n",
    "y_train = df2.iloc[:, 1].values\n",
    "\n",
    "def replace_contractions(sentence):\n",
    "    \"\"\"Replace contractions in string of text\"\"\"\n",
    "    return contractions.fix(sentence)\n",
    "\n",
    "  \n",
    "def words_list(sample):\n",
    "    words = []\n",
    "    \"\"\"Tokenising the corpus\"\"\"\n",
    "    for i in sample: \n",
    "        temp = []\n",
    "        for j in word_tokenize(i):\n",
    "            temp.append(j.lower())\n",
    "        temp = normalize(temp)\n",
    "        words.append(temp)\n",
    "    return words\n",
    "\n",
    "  \n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "  \n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "  \n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "\"\"\"def replace_numbers(words):\n",
    "    Replace all interger occurrences in list of tokenized words with textual representation\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "  \n",
    "def lemmatize_words(words):\n",
    "    \"\"\"Lemmatize the words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word)\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    " \n",
    "def normalize(words):\n",
    "    \"\"\"This is the main function which takes all other functions to pre-process the data given\"\"\"\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_stopwords(words)\n",
    "    lemmatize_words(words)\n",
    "    return words\n",
    "\n",
    "\n",
    "vocab_size = 100000\n",
    "# cut texts after this number of words\n",
    "# (among top max_features most common words)\n",
    "maxlen = 130\n",
    "batch_size = 32\n",
    "word_list = words_list(X_train)\n",
    "X_train = []\n",
    "for list_of_words in word_list:\n",
    "    sentence = ' '.join(x for x in list_of_words)\n",
    "    X_train.append(sentence)\n",
    "X_train, y_train = shuffle(X_train, y_train)\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(X_train)\n",
    "encoded_word_list = t.texts_to_sequences(X_train)\n",
    "X_train = pad_sequences(encoded_word_list, maxlen=maxlen, padding='post')\n",
    "\n",
    "\n",
    "#Managing the y vector\n",
    "y_train = to_categorical(y_train, num_classes = 10)\n",
    "print(y_train[0])\n",
    "\n",
    "#Regularization\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "\n",
    "#Constructing and training the model\n",
    "model = Sequential()\n",
    "e = Embedding(input_dim = vocab_size, output_dim = 100, input_length = X_train.shape[1], dropout=0.2)  #130*100 Output\n",
    "model.add(e)\n",
    "tensors = model.add(Bidirectional(LSTM(128, return_sequences = False, activation = 'tanh')))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation = 'softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, epochs=3, batch_size=batch_size, verbose=1, validation_split=0.1, callbacks=[es, mc])\n",
    "\n",
    "# load the saved model\n",
    "#saved_model = load_model('best_model.h5')\n",
    "\n",
    "#Plotting the results\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "#Testing\n",
    "dataset2 = pd.read_csv('../input/training/data_combined_train.csv')\n",
    "X_test_init = dataset2.iloc[:, 0:1].values\n",
    "y_test = dataset2.iloc[:, 1].values\n",
    "X_test = []\n",
    "for i in X_test_init:\n",
    "    for j in i:\n",
    "        X_test.append(str(j))\n",
    "word_list = words_list(X_test)\n",
    "X_test = []\n",
    "for list_of_words in word_list:\n",
    "    sentence = ' '.join(x for x in list_of_words)\n",
    "    X_test.append(sentence)\n",
    "encoded_word_list = t.texts_to_sequences(X_test)\n",
    "X_test = pad_sequences(encoded_word_list, maxlen=maxlen, padding='post')\n",
    "#Managing the y vector\n",
    "y_test = to_categorical(y_test, num_classes = 10)\n",
    "score, acc = model.evaluate(X_test, y_test, batch_size = batch_size, verbose = 1)\n",
    "print(\"The accuracy of the model on the test set is: \", acc, score)\n",
    "\n",
    "#Confusion Matrix\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = [ np.argmax(t) for t in y_pred ]\n",
    "y_test_non_category = [ np.argmax(t) for t in y_test]\n",
    "from sklearn.metrics import confusion_matrix\n",
    "conf_mat = confusion_matrix(y_test_non_category, y_pred)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
